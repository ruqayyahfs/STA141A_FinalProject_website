---
title: "Methods and Results"
output: bookdown::html_document2
---

```{css, echo =FALSE}
h1.title {
  font-size: 38px;
  color: mediumslateblue;
  text-align: default;
}
```

To answer our first research question, we implemented a multiple regression linear model to determine if any of the variables in our data set are viable predictors of `score`. We fitted an initial model (named model1) that regressed score on all of the variables depicted in figure 1 from "Data Description", except for the variable `budget_cat`, which was only created for data exploration purposes. After running the model and analyzing the t-test results via the `summary()` function, we found that while some levels of the variable categorical `genre` are deemed significant predictors of `score` (e.g. `drama`, `animation`), the majority have very high p-values and are not deemed significant predictors (e.g. `thriller`, `mystery`). `Month` is not a significant predictor, nor `gross`, nor `return_prop`.

We then performed a residual analysis on model1. See figure \@ref(fig:residualsPlots) (LEFT). From the Residuals vs. Fitted plot, the residuals are not linear and exhibit heteroskedasticity. The QQ plot suggests that the residuals are not normally distributed, as the lower and higher standardized residuals do not line up with the theoretical quantiles. We also see some outliers, as well as one particularly high leverage point (i.e. observation 2817 (The 2007 movie "Paranormal Activity")).

Next, using a step-wise model selection procedure with $R^2_{adj}$ as a selection criterion, we selected a model that produced the maximum value for $R^2_{adj}$. We chose to use $R^2_{adj}$ as a criterion, rather than $R^2$ because $R^2_{adj}$ is more appropriate for comparing models with different numbers of predictors. $R^2_{adj}$ will only increase for more predictors, if those predictors allow the model to explain more of the variance in the data. This method will implement partial F-tests and then eliminate one-by-one the least significant predictor. In this case, per both backward and forward elimination, the suggested model is `score` ~ `ratingR` + `runtime` + `genreAnimation` + `genreBiography` + `genreDrama` + `genreHorror` + `votes` + `budget`. 

Since only 1 of 8 levels for `rating` is deemed significant, and only 4 of 15 levels for `genre` is deemed significant, we dropped these predictors along with `return_prop`, `gross`, `year` and `month` from the model, thus creating model2. For model2, we found that all the included predictors were significant; however, the residuals exhibited the same issues as those from model1. Since the residuals were nonlinear and heteroskedastic, model2 was a good candidate for a BoxCox transformation. We created and examined a BoxCox plot which had its maximum value near $\lambda = 2$, suggesting we raise $\hat{score}$ to the 2nd power. We fit the new model (model3) as such:


$$\hat{\text{score}} = \sqrt{\hat\beta_0 + \hat\beta_{\text{runtime}} \cdot \text{runtime} + \hat\beta_{\text{votes}} \cdot \text{votes} + \hat\beta_{\text{budget}} \cdot \text{budget}}$$

With model3, our residual plots began to improve and our $R^2_{adj}$ increased. Yet, we had some high leverage points. We removed them from the model, fit a new model, repeated our analysis, found new high leverage points, and removed those as well. At this point, we decided it would be inappropriate to continue to truncate the data. Another BoxCox transformation was implemented and the final model was reached:

$$\hat{\text{score}} = \sqrt[4]{\hat\beta_0 + \hat\beta_{\text{runtime}} \cdot \text{runtime} + \hat\beta_{\text{votes}} \cdot \text{votes} + \hat\beta_{\text{budget}} \cdot \text{budget}}$$

The final model achieves $R^2_{adj} = 0.4844$, which means that if this model is appropriate, the model above explains $48.44\%$ of the variance of `score`. The residual plots of the final model are not perfect but show significant improvement from the initial model. From Figure \@ref(fig:residualsPlots) (RIGHT): linearity was improved, not quite linear but *much* closer. Homoskedasticity appears more likely. Residual quantiles are quite close to theoretical quantiles, and could pass for normality. We conducted formal tests with the following results: Shapiro-Wilk test concluded that our residuals were not normally distributed; Box-Pierce test indicated the residuals were independently distributed; and Breusch-Pagan test concluded the residuals are not quite homoskedastic.

```{r residuals1, eval = FALSE, fig.cap='Residual Plots for Initial Model', echo=FALSE,warning=FALSE,out.width='60%',out.height='70%'}
# LEGACY
knitr::include_graphics("plots/residuals1.png")
```

```{r residualsFinal, eval = FALSE, fig.cap='Residual Plots for Final Model', echo=FALSE,warning=FALSE,out.width='60%',out.height='70%'}
# LEGACY
knitr::include_graphics("plots/residualsFinal.png")
```

```{r residualsPlots, fig.cap='Full model residuals plots (left) and final model residuals plots (right)', fig.show = 'hold', fig.align = 'center', echo=FALSE,warning=FALSE,out.width='49%',out.height='65%'}
knitr::include_graphics(c("plots/residuals1.png", "plots/residualsFinal.png"))
```

## <span style="color: mediumslateblue;">Cluster Analysis</span> {#ca}


To investigate our second research question, we implemented hierarchical clustering methods to group movies together that contain similar attributes so that we can recommend similar movies. This data set contains a large number of categorical variables `rating`, `genre`, and `month` that might be relevant to the aggregation of movies that are alike. As these values are categorical values, a Euclidean distance is not suitable for evaluating the degree of similarity between observations. As a result, we relied on a different distance measure to achieve this classification, known as Gower distance. 

The `daisy()` function with parameter `metric = "gower"` from the package `cluster` computes the Gower's distance (as a dissimilarity object) between units in a data set or between observations in two distinct data sets. It is useful in cases where records contain combinations of logical, numerical, categorical or text data. (Dâ€™Orazio 2021) We used this method to create hierarchical clusters, and plot dendrogram of the results (Figure \@ref(fig:dendPlots)).

The Full Hierarchical Clustering in figure \@ref(fig:dendPlots) (RIGHT) is the full clustering with all the variables (except `budget_cat` since we used `budget` and did not want to essentially reference the same variable twice). An additional cluster model was created by removing some of the variables that likely would not have as significant an impact on how the consumer perceived a film. This emphasized some of the qualitative aspects. The result of this Reduced Variable Clustering in figure \@ref(fig:dendPlots) (LEFT) only has the variables `rating`, `genre`, `year`, `score`, and `budget`. The Reduced Variable Hierarchical Clustering ended up with 6 relatively clearly defined categories (dark blue line visualizes cut of dendrogram), while the Full Clustering had 3 primary clusters.

```{r dendPlots, fig.cap='Colored dendrogram plots of full and reduced variable hierarchial clustering', fig.show = 'hold', fig.align = 'center', echo=FALSE,warning=FALSE,out.width='49%',out.height='65%'}
knitr::include_graphics(c("plots/dend1.png", "plots/dend2.png"))
```

Cutting and returning the list of movies in the each of the clusters of both dendrograms allows us to view the titles of the movies in those clusters. By doing this we are able to visually see the groupings by movie name and infer how they may be clustered differently based on which variables are included in the model. To further explore where these difference may be coming from, we created a function that returned a data frame (displayed as a table with `knitr`) with the means of numeric variables and the most frequent value and proportion of that value of categorical variables in each cluster of the Full Hierarchical Clustering model and the Reduced Variables Hierarchical Clustering models. The function also returns the total number of values in each of the clusters.

```{r fullhclustMeans, fig.cap='Mean and Proportions of Full Hierarchial Clustering', fig.show = 'hold', fig.align = 'center', echo=FALSE,warning=FALSE,out.width='90%',out.height='65%'}
knitr::include_graphics("plots/full_hclust_means2.png")
```

<br></br>

```{r reducedhclustMeans, fig.cap='Mean and Proportions of Reduced Hierarchial Clustering', fig.show = 'hold', fig.align = 'center', echo=FALSE,warning=FALSE,out.width='90%',out.height='65%'}
knitr::include_graphics("plots/reduced_hclust_means2.png")
``` 

Including the number of values allows us to associate cluster numbers to the plots above. 

The most interesting finding within the full cluster analysis is the significant difference between the red clustering on the Full Clustering plot and the other two clusters on that same plot. This small cluster of 74 observations (labelled Cluster 3 in Figure \@ref(fig:fullhclustMeans)). These represent newer, higher budget, action films, commonly rated PG-13. Investigating which movies belong to this cluster, we find "Avatar", "Avengers: Age of Ultron", some of the Star Wars films, etc. Clearly this cluster is a really dense group of very similar, high budget, high grossing, more highly rated films. These are a very specific aggregation of "action blockbuster" films.

The second cluster analysis on reduced variables resulted in six clusters of far more meaningful differences on a consumer level. In a similar way to the "action blockbuster" films described in the previous paragraph, many of the clusters aggregated in a way that can be defined as a phrase to a prospective consumer. Cluster 5 in Figure \@ref(fig:reducedhclustMeans (light blue in the Reduced Variable Dendrogram) for instance contains almost entirely animated films for children which included Disney movies such as "The Lion King" and "Mulan."
